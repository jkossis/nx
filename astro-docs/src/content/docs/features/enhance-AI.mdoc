---
title: 'Enhance Your LLM'
description: 'Learn how Nx enhances your AI assistant by providing rich workspace metadata, architectural insights, and project relationships to make your LLM smarter and more context-aware.'
sidebar:
  order: 3
filter: 'type:Features'
---

{% youtube src="https://youtu.be/dRQq_B1HSLA" title="We Just Shipped the Monorepo MCP for Copilot" /%}

Monorepos [provide an ideal foundation for AI-powered development](https://nx.dev/blog/nx-and-ai-why-they-work-together), enabling cross-project reasoning and code generation. However, without proper context, **LLMs struggle to understand your workspace architecture**, seeing only individual files rather than the complete picture.

Nx transforms your AI assistant by providing rich workspace metadata that enables it to:

- **Understand your workspace architecture** - Project relationships, dependencies, and how code flows through your monorepo
- **Identify project owners** - Team responsibilities and who to consult for changes
- **Access up-to-date Nx documentation** - Accurate guidance without hallucinations
- **Leverage code generators** - Consistent scaffolding with intelligent defaults
- **Connect to your CI pipeline** - Debug failures and propose fixes automatically
- **Analyze change impact** - Understand how modifications affect dependent projects

The goal is to transform your AI assistant from a generic code helper into an architecturally-aware collaborator that understands your specific workspace structure and can make intelligent, context-aware decisions.

## How Nx MCP Enhances Your LLM

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is an open standard that enables AI models to interact with your development environment through a standardized interface. Nx implements an MCP server via the [Nx Console](/docs/getting-started/editor-setup) that exposes workspace metadata to compatible AI assistants like GitHub Copilot, Claude, and others.

With the Nx MCP server, your AI assistant gains a "map" of your entire system being able to go from just reasoning at the file level to seeing the higher-level picture. This allows the LLM to move between different abstraction levels - from high-level architecture down to specific implementation details:

![Different abstraction levels](../../../assets/features/nx-ai-abstraction-levels.avif)

The Nx MCP server exposes tools for workspace analysis, code generation, documentation lookup, and CI/CD analytics. For a complete list of available tools and their descriptions, see the [Nx MCP Server Reference](/docs/reference/nx-mcp#available-tools).

## Setting Up Nx MCP

### Automatic Setup (Recommended)

The fastest way to configure Nx for AI development is with the automatic setup command:

```shell
npx nx configure-ai-agents
```

This interactive command will:

- Detect your IDE and AI tools
- Configure the Nx MCP server for your chosen tools
- Create AI agent configuration files (`CLAUDE.md`, `AGENTS.md`, etc.) with workspace-specific guidelines

### IDE Setup

For VS Code, Cursor, and JetBrains IDE users, install [Nx Console](/docs/getting-started/editor-setup) from the marketplace. You'll receive a notification to "Improve Copilot/AI agent with Nx-specific context" - click "Yes" to configure the MCP server.

![VS Code showing the Nx MCP installation prompt](../../../assets/features/copilot-mcp-install.avif)

If you miss the notification, run the `nx.configureMcpServer` command from the command palette (`Ctrl/Cmd + Shift + P`), or `Nx: Setup MCP Server` in JetBrains IDEs (`Ctrl/Cmd + Shift + A`).

### Manual Setup by Client

{% tabs %}
{% tabitem label="Claude Code" %}

```shell
claude mcp add nx-mcp npx nx-mcp@latest
```

{% /tabitem %}
{% tabitem label="VS Code" %}

```shell
code --add-mcp '{"name":"nx-mcp","command":"npx","args":["nx-mcp"]}'
```

Or configure in your VS Code settings, or use the Nx Console extension for automatic setup.

{% /tabitem %}
{% tabitem label="mcp.json" %}

For clients that support `mcp.json` configuration:

```json
{
  "servers": {
    "nx-mcp": {
      "type": "stdio",
      "command": "npx",
      "args": ["nx-mcp@latest"]
    }
  }
}
```

{% /tabitem %}
{% tabitem label="Other Clients" %}

Configure your client to run the `npx nx-mcp` command to start the Nx MCP server. For HTTP/SSE transport:

```shell
npx nx-mcp@latest --transport sse --port 9921
```

{% /tabitem %}
{% /tabs %}

For advanced configuration options including tool filtering and transport modes, see the [Nx MCP Server Reference](/docs/reference/nx-mcp).

## Powerful Use Cases

### Understanding Your Workspace Architecture

{% youtube src="https://youtu.be/RNilYmJJzdk" title="Nx Just Made Your LLM Way Smarter" /%}

Ask your AI assistant about your workspace structure and get detailed, accurate responses about projects, their types, and relationships:

```text
What is the structure of this workspace?
How are the projects organized?
```

With Nx MCP, your AI assistant can:

- Identify applications and libraries in your workspace
- Understand project categorization through tags
- Recognize technology types (feature, UI, data-access)
- Determine project ownership and team responsibilities

![Example of LLM understanding project structure](../../../assets/features/nx-ai-example-project-data.avif)

You can also get informed suggestions about where to implement new functionality:

```text
Where should I implement a feature for adding products to cart?
```

![Example of LLM providing implementation guidance](../../../assets/features/nx-ai-example-data-access-feature.avif)

Learn more about workspace architecture understanding in our blog post [Nx Just Made Your LLM Way Smarter](https://nx.dev/blog/nx-just-made-your-llm-smarter).

### Instant CI Failure Resolution

{% youtube src="https://youtu.be/fPqPh4h8RJg" title="Connect Your Editor, CI and LLMs" /%}

When a CI build fails, Nx Console can notify you directly in your editor:

![Nx Console shows the notification of the CI failure](../../../assets/features/ci-notification.avif)

Your AI assistant can then:

1. Access detailed information from Nx Cloud about the failed build
2. Analyze your git history to understand what changed in your PR
3. Understand the error context and affected files
4. Help implement the fix right in your editor

This integration dramatically improves the development velocity because you get immediately notified when an error occurs, you don't even have to leave your editor to understand what broke, and the LLM can help you implement or suggest a possible fix.

Learn more about CI integration in our blog post [Save Time: Connecting Your Editor, CI and LLMs](https://nx.dev/blog/nx-editor-ci-llm-integration).

### Smart Code Generation with AI-Enhanced Generators

{% youtube src="https://youtu.be/PXNjedYhZDs" title="Enhancing Nx Generators with AI" /%}

Nx generators provide predictable code scaffolding, while AI adds intelligence and contextual understanding. Instead of having the AI generate everything from scratch, you get the best of both worlds:

```text
Create a new React library into the packages/orders/feat-cancel-orders folder
and call the library with the same name of the folder structure. Afterwards,
also connect it to the main shop application.
```

Your AI assistant will:

1. Identify the appropriate generator and its parameters
2. Open the Nx Console Generate UI with preset values
3. Let you review and customize the options
4. Execute the generator and help integrate the new code with your existing projects

![LLM invoking the Nx generate UI](../../../assets/features/llm-nx-generate-ui.avif)

This approach ensures consistent code that follows your organization's best practices while still being tailored to your specific needs. Learn more about AI-enhanced generators in our blog post [Enhancing Nx Generators with AI](https://nx.dev/blog/nx-generators-ai-integration).

### Documentation-Aware Configuration

{% youtube src="https://youtu.be/V2W94Sq_v6A?si=aBA-eppEw0fHrh5O&t=388" title="Making Cursor Smarter with an MCP Server" /%}

Get accurate guidance on Nx configuration without worrying about hallucinations or outdated information:

```text
Can you configure Nx release for the packages of this workspace?
Update nx.json with the necessary configuration using conventional commits
as the versioning strategy.
```

The AI assistant will:

1. Query the Nx docs for the latest information on release configuration
2. Understand your workspace structure to identify packages
3. Generate the correct configuration based on your specific needs
4. Apply the changes to your nx.json file

Learn more about documentation-aware configuration in our blog post [Making Cursor Smarter with an MCP Server For Nx Monorepos](https://nx.dev/blog/nx-made-cursor-smarter).

### Cross-Project Dependency Analysis

{% youtube src="https://youtu.be/dRQq_B1HSLA?si=lhHsjRvwgijC1IL8&t=186" title="Nx MCP Now Available for VS Code Copilot" /%}

Understand the impact of changes across your monorepo with questions like:

```text
If I change the public API of feat-product-detail, which other projects
might be affected by that change?
```

Your AI assistant can:

- Analyze the project graph to identify direct and indirect dependencies
- Visualize affected projects using the `nx_visualize_graph` tool
- Suggest strategies for refactoring that minimize impact
- Identify which teams would need to be consulted for major changes

This architectural awareness is particularly powerful in larger monorepos where understanding project relationships is crucial for making informed development decisions.

Learn more about dependency analysis in our blog post [Nx MCP Now Available for VS Code Copilot](https://nx.dev/blog/nx-mcp-vscode-copilot).

## Best Practices for AI Development

### Use Generators Over Raw Code Generation

When creating new projects, libraries, or components, guide your AI to use Nx generators rather than generating files from scratch:

```text
# Good: Uses generators for consistent structure
Create a new React library using the @nx/react:library generator

# Less ideal: AI generates everything manually
Create a new React library with these files...
```

Generators ensure your code follows workspace conventions, includes proper configuration, and integrates correctly with the project graph.

### Leverage Project Tags for Context

Well-organized project tags help your AI understand code boundaries and suggest appropriate locations for new features:

```text
Where should I add a new data-access library for user authentication?
```

With proper tags like `scope:shared`, `type:data-access`, or `team:platform`, the AI can make informed recommendations.

### Ask Architectural Questions First

Before implementing features, use the AI's workspace awareness to understand the current architecture:

```text
# Understand before implementing
What libraries does the checkout feature currently depend on?
Which teams own the projects I'll need to modify?
```

### Combine CI Context with Local Development

When builds fail, let the AI analyze both CI output and your local changes:

```text
My CI build failed. What changed in my PR that might have caused
the type error in the shared-ui library?
```

## What's Next

- **[Self-Healing CI](/docs/features/ci-features/self-healing-ci)** - Automatically fix CI failures with AI-proposed solutions
- **[Nx MCP Server Reference](/docs/reference/nx-mcp)** - Complete tool reference and advanced configuration
- **[Editor Setup](/docs/getting-started/editor-setup)** - Install Nx Console for the best AI integration experience
